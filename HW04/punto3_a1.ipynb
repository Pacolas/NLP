{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUNTO 3 - ARQUITECTURA 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descarga de librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 9 libros en la carpeta /books\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "books_folder = './books/'\n",
    "\n",
    "\n",
    "book_files = [f for f in os.listdir(books_folder) if f.endswith('.txt')]\n",
    "\n",
    "names = []\n",
    "books_texts = []\n",
    "for book_file in book_files:\n",
    "    with open(os.path.join(books_folder, book_file), 'r', encoding='utf-8') as file:\n",
    "        books_texts.append(file.read())\n",
    "        names.append(book_file.split('_')[0])\n",
    "\n",
    "print(f'Se encontraron {len(book_files)} libros en la carpeta /books')\n",
    "processed_books = [\" \".join(simple_preprocess(text)) for text in books_texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir textos en fragmentos con su respectiva etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_fragments(text, fragment_size=150):\n",
    "    words = text.split()\n",
    "    fragments = [words[i:i + fragment_size] for i in range(0, len(words), fragment_size)]\n",
    "    return [' '.join(fragment) for fragment in fragments]\n",
    "\n",
    "\n",
    "fragment_size = 150  \n",
    "fragmented_books = []\n",
    "fragment_labels = []\n",
    "\n",
    "for i, book_text in enumerate(books_texts):\n",
    "    fragments = split_into_fragments(book_text, fragment_size=fragment_size)\n",
    "    fragmented_books.extend(fragments)\n",
    "    fragment_labels.extend([names[i]] * len(fragments))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision de fragmentos por etiqueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7553\n",
      "{'tolstoy': 4544, 'forster': 1520, 'vonarnin': 1489}\n"
     ]
    }
   ],
   "source": [
    "print(len(fragment_labels))\n",
    "dic = {}\n",
    "for i in fragment_labels:\n",
    "    dic[i] = dic.get(i, 0)+ 1\n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(fragmented_books)\n",
    "X = tokenizer.texts_to_sequences(fragmented_books)\n",
    "\n",
    "X = pad_sequences(X, maxlen=fragment_size)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(fragment_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division entre test y train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /Users/nicolasp/Library/Python/3.8/lib/python/site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/nicolasp/Library/Python/3.8/lib/python/site-packages (from imbalanced-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/nicolasp/Library/Python/3.8/lib/python/site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/nicolasp/Library/Python/3.8/lib/python/site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/nicolasp/Library/Python/3.8/lib/python/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nicolasp/Library/Python/3.8/lib/python/site-packages (from imbalanced-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de la matriz de embeddings pre entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings de tama単o 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec  \n",
    "embedding_model_3 = Word2Vec.load('Books_300_EMF.model')\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "embedding_dim_3 = embedding_model_3.vector_size  \n",
    "embedding_matrix_3 = np.zeros((vocab_size, embedding_dim_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in embedding_model_3.wv:\n",
    "        embedding_matrix_3[idx] = embedding_model_3.wv[word]\n",
    "    else:\n",
    "        embedding_matrix_3[idx] = np.zeros(embedding_dim_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding de tama単o 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec  \n",
    "embedding_model_2 = Word2Vec.load('Books_200_EMF.model')\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "embedding_dim_2 = embedding_model_2.vector_size  \n",
    "embedding_matrix_2 = np.zeros((vocab_size, embedding_dim_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in embedding_model_2.wv:\n",
    "        embedding_matrix_2[idx] = embedding_model_2.wv[word]\n",
    "    else:\n",
    "        embedding_matrix_2[idx] = np.zeros(embedding_dim_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings de tama単o 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec  \n",
    "embedding_model = Word2Vec.load('Books_100_EMF.model')\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "embedding_dim = embedding_model.vector_size  \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in embedding_model.wv:\n",
    "        embedding_matrix[idx] = embedding_model.wv[word]\n",
    "    else:\\\n",
    "        embedding_matrix[idx] = np.zeros(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de la red feed-forward: Arquitectura 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_dim_3, \n",
    "                    weights=[embedding_matrix_3],  \n",
    "                    input_length=fragment_size,  \n",
    "                    trainable=False))  \n",
    "\n",
    "model_3.add(Flatten()) \n",
    "model_3.add(Dense(128, activation='relu')) \n",
    "model_3.add(Dense(64, activation='relu'))   \n",
    "model_3.add(Dense(len(set(names)), activation='softmax'))  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forster', 'tolstoy', 'vonarnin'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3635, 1: 3635, 2: 3635}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_dim_2, \n",
    "                    weights=[embedding_matrix_2],  \n",
    "                    input_length=fragment_size,  \n",
    "                    trainable=False))  \n",
    "\n",
    "model_2.add(Flatten()) \n",
    "model_2.add(Dense(128, activation='relu')) \n",
    "model_2.add(Dense(64, activation='relu'))   \n",
    "model_2.add(Dense(len(set(names)), activation='softmax'))  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix],  \n",
    "                    input_length=fragment_size,  \n",
    "                    trainable=False))  \n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(128, activation='relu')) \n",
    "model.add(Dense(64, activation='relu'))   \n",
    "model.add(Dense(len(set(names)), activation='softmax'))  #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 1.0952 - accuracy: 0.3963 - val_loss: 1.0488 - val_accuracy: 0.4488\n",
      "Epoch 2/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.9924 - accuracy: 0.5023 - val_loss: 0.9507 - val_accuracy: 0.5468\n",
      "Epoch 3/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.8418 - accuracy: 0.6121 - val_loss: 0.8524 - val_accuracy: 0.6139\n",
      "Epoch 4/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.6999 - accuracy: 0.6943 - val_loss: 0.7935 - val_accuracy: 0.6465\n",
      "Epoch 5/100\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 0.5615 - accuracy: 0.7699 - val_loss: 0.7233 - val_accuracy: 0.6898\n",
      "Epoch 6/100\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 0.4660 - accuracy: 0.8138 - val_loss: 0.6817 - val_accuracy: 0.7286\n",
      "Epoch 7/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.3907 - accuracy: 0.8519 - val_loss: 0.6609 - val_accuracy: 0.7569\n",
      "Epoch 8/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.3197 - accuracy: 0.8801 - val_loss: 0.6585 - val_accuracy: 0.7635\n",
      "Epoch 9/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.2636 - accuracy: 0.9043 - val_loss: 0.6677 - val_accuracy: 0.7803\n",
      "Epoch 10/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.2249 - accuracy: 0.9178 - val_loss: 0.7424 - val_accuracy: 0.7781\n",
      "Epoch 11/100\n",
      "341/341 [==============================] - 2s 5ms/step - loss: 0.2033 - accuracy: 0.9245 - val_loss: 0.7102 - val_accuracy: 0.7935\n",
      "Epoch 12/100\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 0.1872 - accuracy: 0.9325 - val_loss: 0.7301 - val_accuracy: 0.7983\n",
      "Epoch 13/100\n",
      "341/341 [==============================] - 2s 6ms/step - loss: 0.1533 - accuracy: 0.9452 - val_loss: 0.7750 - val_accuracy: 0.8023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x317776100>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "341/341 [==============================] - 4s 10ms/step - loss: 1.1027 - accuracy: 0.3989 - val_loss: 1.0506 - val_accuracy: 0.4386\n",
      "Epoch 2/100\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 0.9843 - accuracy: 0.5094 - val_loss: 0.9604 - val_accuracy: 0.5314\n",
      "Epoch 3/100\n",
      "341/341 [==============================] - 4s 11ms/step - loss: 0.8279 - accuracy: 0.6201 - val_loss: 0.8531 - val_accuracy: 0.6025\n",
      "Epoch 4/100\n",
      "341/341 [==============================] - 4s 12ms/step - loss: 0.6813 - accuracy: 0.7046 - val_loss: 0.7642 - val_accuracy: 0.6645\n",
      "Epoch 5/100\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 0.5595 - accuracy: 0.7717 - val_loss: 0.7032 - val_accuracy: 0.7022\n",
      "Epoch 6/100\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 0.4549 - accuracy: 0.8193 - val_loss: 0.7110 - val_accuracy: 0.7268\n",
      "Epoch 7/100\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 0.3812 - accuracy: 0.8514 - val_loss: 0.6633 - val_accuracy: 0.7649\n",
      "Epoch 8/100\n",
      "341/341 [==============================] - 3s 9ms/step - loss: 0.3105 - accuracy: 0.8815 - val_loss: 0.6848 - val_accuracy: 0.7708\n",
      "Epoch 9/100\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 0.2548 - accuracy: 0.9058 - val_loss: 0.7916 - val_accuracy: 0.7660\n",
      "Epoch 10/100\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 0.2423 - accuracy: 0.9098 - val_loss: 0.7204 - val_accuracy: 0.7895\n",
      "Epoch 11/100\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 0.1857 - accuracy: 0.9343 - val_loss: 0.7059 - val_accuracy: 0.8034\n",
      "Epoch 12/100\n",
      "341/341 [==============================] - 3s 10ms/step - loss: 0.1771 - accuracy: 0.9344 - val_loss: 0.7708 - val_accuracy: 0.7979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x322088e80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_2 = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model_2.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6633 - accuracy: 0.7649\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "341/341 [==============================] - 5s 13ms/step - loss: 1.1280 - accuracy: 0.3873 - val_loss: 1.0537 - val_accuracy: 0.4474\n",
      "Epoch 2/100\n",
      "341/341 [==============================] - 5s 15ms/step - loss: 0.9730 - accuracy: 0.5170 - val_loss: 0.9741 - val_accuracy: 0.5163\n",
      "Epoch 3/100\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 0.8463 - accuracy: 0.6086 - val_loss: 0.8893 - val_accuracy: 0.5838\n",
      "Epoch 4/100\n",
      "341/341 [==============================] - 5s 14ms/step - loss: 0.7053 - accuracy: 0.6920 - val_loss: 0.7772 - val_accuracy: 0.6590\n",
      "Epoch 5/100\n",
      "341/341 [==============================] - 7s 20ms/step - loss: 0.5792 - accuracy: 0.7619 - val_loss: 0.7157 - val_accuracy: 0.6978\n",
      "Epoch 6/100\n",
      "341/341 [==============================] - 6s 17ms/step - loss: 0.4886 - accuracy: 0.8024 - val_loss: 0.7084 - val_accuracy: 0.7103\n",
      "Epoch 7/100\n",
      "341/341 [==============================] - 6s 16ms/step - loss: 0.4017 - accuracy: 0.8391 - val_loss: 0.7150 - val_accuracy: 0.7316\n",
      "Epoch 8/100\n",
      "341/341 [==============================] - 6s 17ms/step - loss: 0.3542 - accuracy: 0.8584 - val_loss: 0.6702 - val_accuracy: 0.7580\n",
      "Epoch 9/100\n",
      "341/341 [==============================] - 7s 20ms/step - loss: 0.2892 - accuracy: 0.8866 - val_loss: 0.6450 - val_accuracy: 0.7792\n",
      "Epoch 10/100\n",
      "341/341 [==============================] - 6s 19ms/step - loss: 0.2500 - accuracy: 0.9070 - val_loss: 0.7335 - val_accuracy: 0.7763\n",
      "Epoch 11/100\n",
      "341/341 [==============================] - 7s 20ms/step - loss: 0.2275 - accuracy: 0.9166 - val_loss: 0.7016 - val_accuracy: 0.7943\n",
      "Epoch 12/100\n",
      "341/341 [==============================] - 6s 18ms/step - loss: 0.1884 - accuracy: 0.9294 - val_loss: 0.7304 - val_accuracy: 0.8016\n",
      "Epoch 13/100\n",
      "341/341 [==============================] - 6s 18ms/step - loss: 0.1688 - accuracy: 0.9395 - val_loss: 0.7969 - val_accuracy: 0.8045\n",
      "Epoch 14/100\n",
      "341/341 [==============================] - 5s 16ms/step - loss: 0.1501 - accuracy: 0.9460 - val_loss: 0.8846 - val_accuracy: 0.7884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x358474d60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_3 = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model_3.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6450 - accuracy: 0.7792\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados  y metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 150, 100)          3507800   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 15000)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               1920128   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5436379 (20.74 MB)\n",
      "Trainable params: 1928579 (7.36 MB)\n",
      "Non-trainable params: 3507800 (13.38 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81       909\n",
      "           1       0.72      0.58      0.64       909\n",
      "           2       0.77      0.88      0.82       909\n",
      "\n",
      "    accuracy                           0.76      2727\n",
      "   macro avg       0.76      0.76      0.76      2727\n",
      "weighted avg       0.76      0.76      0.76      2727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 150, 200)          7015600   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 30000)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               3840128   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10864179 (41.44 MB)\n",
      "Trainable params: 3848579 (14.68 MB)\n",
      "Non-trainable params: 7015600 (26.76 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_2.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.82       909\n",
      "           1       0.77      0.53      0.63       909\n",
      "           2       0.77      0.87      0.81       909\n",
      "\n",
      "    accuracy                           0.76      2727\n",
      "   macro avg       0.77      0.76      0.76      2727\n",
      "weighted avg       0.77      0.76      0.76      2727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings tama単o 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 300)          10523400  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 45000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               5760128   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16291979 (62.15 MB)\n",
      "Trainable params: 5768579 (22.01 MB)\n",
      "Non-trainable params: 10523400 (40.14 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_3.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.85      0.81       909\n",
      "           1       0.74      0.60      0.67       909\n",
      "           2       0.81      0.88      0.85       909\n",
      "\n",
      "    accuracy                           0.78      2727\n",
      "   macro avg       0.78      0.78      0.77      2727\n",
      "weighted avg       0.78      0.78      0.77      2727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
